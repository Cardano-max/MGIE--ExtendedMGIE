# EntendedMGIE - MLLM-Guided Image Editing with Progressive Feature Blending and Cross-Attention Masking

Welcome to the official implementation of the enhanced MLLM-Guided Image Editing (MGIE) framework! This cutting-edge project combines Progressive Feature Blending (PFB) and Cross-Attention Masking (CAM) techniques to produce high-quality, semantically aligned, and controllable text-driven image edits. Leveraging the expressive capabilities of Multimodal Large Language Models (MLLMs), our framework ensures realistic and coherent edited images, guided by sophisticated visual-aware instructions.

## Table of Contents

- [Introduction](#introduction)
- [Paper](#paper)
- [Code](#code)
- [Results](#results)
- [Getting Started](#getting-started)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
- [Usage](#usage)
- [Contributing](#contributing)
- [License](#license)
- [Acknowledgements](#acknowledgements)

## Introduction

The MLLM-Guided Image Editing (MGIE) framework is designed to revolutionize text-driven image editing. By integrating MLLMs to generate detailed instructions, our framework guides the editing process to achieve outstanding results. While the original MGIE framework was impressive, our enhancements with Progressive Feature Blending (PFB) and Cross-Attention Masking (CAM) take it to the next level.

### Key Enhancements

- **Progressive Feature Blending (PFB):** Seamlessly integrates MLLM-generated content with the original image across multiple feature levels, ensuring visual coherence and consistency.
- **Cross-Attention Masking (CAM):** Provides precise control over the editing process by focusing the influence of specific text tokens on desired image regions.

Extensive experiments and analyses demonstrate that our enhanced MGIE framework outperforms previous methods in terms of visual quality, semantic alignment, and faithfulness to the original image.

## Paper

For a deep dive into our methodology, experiments, and results, check out our paper titled "Enhancing MLLM-Guided Image Editing with Progressive Feature Blending and Cross-Attention Masking." The paper is available in the `paper/` directory and offers comprehensive insights into our techniques and their contributions.

## Code

Explore the implementation of the enhanced MGIE framework in the `code/` directory. Our primary implementation is provided in the `mgie_implementation.ipynb` Jupyter notebook, which includes step-by-step instructions for training and testing the framework on various datasets. Ensure you have all dependencies listed in the `requirements.txt` file.

## Results

Check out our `results/` directory to see sample input images and their corresponding edited outputs generated by the enhanced MGIE framework. The `input_images/` subdirectory contains the original images, while the `output_images/` subdirectory showcases the edited versions, highlighting the effectiveness of our framework.

## Getting Started

Ready to dive in? Follow these steps to set up and start using the enhanced MGIE framework.

### Prerequisites

- Python 3.6 or above
- PyTorch 1.9 or above
- CUDA 11.0 or above (for GPU acceleration)

### Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/your-username/ml-mgie-implementation.git
   cd ml-mgie-implementation
   Install the required dependencies:


2. Install the required dependencies:

   ```bash
    pip install -r code/requirements.txt

3. Download the pretrained models and datasets:

   - Place the pretrained MLLM model (e.g., LLaVA-7B) in the `models/` directory.
   - Place the desired datasets (e.g., COCO, CUB, Oxford-102 Flowers) in the `data/` directory.

## Usage

1. Open the `code/mgie_implementation.ipynb` Jupyter notebook.
2. Follow the instructions to train and test the framework on your chosen datasets.
3. Modify the notebook as needed to experiment with different settings and hyperparameters.
4. Provide input images, text prompts, and binary masks (if applicable) to generate edited images.
5. Find your edited images saved in the `results/output_images/` directory.

## Contributing

We welcome contributions! If you encounter issues or have suggestions for improvement, please open an issue or submit a pull request. Adhere to the existing code style and provide detailed explanations of your changes.

## Acknowledgements

We extend our gratitude to the original MGIE framework and PFB-Diff method authors for their foundational work in text-driven image editing. Thanks also to the open-source community for providing essential tools and libraries used in this implementation.


